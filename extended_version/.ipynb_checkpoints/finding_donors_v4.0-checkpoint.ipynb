{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Donors for Charity ML\n",
    "\n",
    "### This is my extended solution for Udacity's Data Scientist Nanodegree first project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version has log correction of capital-gain and capital-loss. The 99999 entries were not removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Original text by Udacity\n",
    "\n",
    "## Description\n",
    "\n",
    "CharityML is a fictitious charity organization located in the heart of Silicon Valley that was established to provide financial support for people eager to learn machine learning. After nearly 32,000 letters were sent to people in the community, CharityML determined that every donation they received came from someone that was making more than $50,000 annually. To expand their potential donor base, CharityML has decided to send letters to residents of California, but to only those most likely to donate to the charity. With nearly 15 million working Californians, CharityML has brought you on board to help build an algorithm to best identify potential donors and reduce overhead cost of sending mail. Your goal will be evaluate and optimize several different supervised learners to determine which algorithm will provide the highest donation yield while also reducing the total number of letters being sent.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In this project, you will employ several supervised algorithms of your choice to accurately model individuals' income using data collected from the 1994 U.S. Census. You will then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. Your goal with this implementation is to construct a model that accurately predicts whether an individual makes more than $50,000. This sort of task can arise in a non-profit setting, where organizations survive on donations.  Understanding an individual's income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with.  While it can be difficult to determine an individual's general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features. \n",
    "\n",
    "The dataset for this project originates from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Census+Income). The datset was donated by Ron Kohavi and Barry Becker, after being published in the article _\"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid\"_. You can find the article by Ron Kohavi [online](https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf). The data we investigate here consists of small changes to the original dataset, such as removing the `'fnlwgt'` feature and records with missing or ill-formatted entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and describing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-39bd4332017c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from data folder\n",
    "data = pd.read_csv('../data/census.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check a sample of the dataframe\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dataset records\n",
    "n_records = data.shape[0]\n",
    "print('Dataset has %d records'% n_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general information about dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset is composed of 45222 records and 14 columns. There are no missing entries. The records are numerical (int64 and float64) or text (object).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-number data summary\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson's correlation coefficient for the numerical variables\n",
    "sns.heatmap(data.corr(), annot=True, cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some of the features are weakly correlated. It is unlikely this will affect the classification outcome.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually inspect all the combinations of numerical variables\n",
    "sns.pairplot(data, hue='income', diag_kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**None of the features follow a Normal distribution. Capital-gain shows a very suspicious line at 99999. Is this some census code for non-declared value? We will need to replace these points. It is also clear that the features are not correlated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 229 entries with capital-gain = 99999.0. That's not a lot.\n",
    "data[data['capital-gain'] == data['capital-gain'].max()]['capital-gain'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of capital-gain before cleaning shows the outliers.\n",
    "plt.hist(data[data['income']=='>50K']['capital-gain']);\n",
    "plt.xlabel('Income > 50K');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the outliers in capital-gain by first turning all outliers into nan\n",
    "#data.loc[data['capital-gain']==99999.0,'capital-gain']  = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then applying fillna. We use mean as replacement method because median returns 0.\n",
    "#data = data.fillna(data[data['income']=='>50K']['capital-gain'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The histogram after replacement looks much better.\n",
    "#plt.hist(data[data['income']=='>50K']['capital-gain']);\n",
    "#plt.xlabel('Income > 50K');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we check pairplot again\n",
    "#sns.pairplot(data, hue='income', diag_kind='hist');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and calculate the correlation coefficient for the modified dataset.\n",
    "#sns.heatmap(data.corr(), annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation between capital-gain and capital loss slightly increased but it still small.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last, we check the data summary for modified dataset.\n",
    "#data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the sequence for data preparation:\n",
    "\n",
    "1-  generate target array by selecting incomes >50K\n",
    "2- \n",
    "\n",
    "Udacity recommended us to use a log transformation with capital-gain and capital-loss. I removed this transformation because it was not anymore needed after the outliers were eliminated from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b80c0ee0947f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# First we define the number of entries with income greater than 50K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mn_greater_50k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'income'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'>50K'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'income'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset has %d individuals earning more than $50,000 annually'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_greater_50k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# First we define the number of entries with income greater than 50K\n",
    "n_greater_50k = data[data['income'] == '>50K']['income'].count()\n",
    "\n",
    "print('Dataset has %d individuals earning more than $50,000 annually' % n_greater_50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then the number of entries with income at most equal to 50K.\n",
    "n_at_most_50k = data[data['income'] == '<=50K']['income'].count()\n",
    "\n",
    "print('Dataset has %d individuals earning at most $50,000 annually' % n_at_most_50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we check for consistency of number of entries\n",
    "n_records == n_greater_50k + n_at_most_50k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and calculate the percentage of entries with income greater than 50K.\n",
    "greater_percent = n_greater_50k / n_records * 100\n",
    "\n",
    "print('{:4.2f}% of the individuals earn more than $50,000 annually'.format(greater_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only 25% of the entries earn more than 50K. This means that the dataset is skewed and accuracy is not a good metric for this problem. I will keep using it but I will take the decisions about the model based on F1 score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate target column from dataset for preprocessing. \n",
    "income_raw = data['income']\n",
    "# define new feature dataset without target column.\n",
    "features_raw = data.drop('income', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe containing transformed skewed features\n",
    "skewed = ['capital-gain', 'capital-loss']\n",
    "features_log_transformed = pd.DataFrame(data = features_raw)\n",
    "# Log-transform skewed features (capital-gain and capital-loss)\n",
    "features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MinMaxScaler object\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to data.info() the numerical variables are:\n",
    "numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "# Create new dataframe with scaled features\n",
    "features_log_min_max_transform = pd.DataFrame(data = features_log_transformed)\n",
    "features_log_min_max_transform[numerical] = scaler.fit_transform(\n",
    "    features_log_transformed[numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect new dataframe\n",
    "features_log_min_max_transform.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding of object features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe with one-hot encoded object features\n",
    "features_final = pd.get_dummies(features_log_min_max_transform) # another option would be sklearn OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of previous dataframe\n",
    "features_log_min_max_transform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of one-hot encoded dataframe\n",
    "features_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map target column from strings to (0,1) range.\n",
    "# Incomes >50K map to 1\n",
    "income = income_raw.map(lambda x: 0 if x=='<=50K' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print list of features after one-hot encoding\n",
    "encoded = list(features_final.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and Split Data\n",
    "\n",
    "So far we removed the outliers from capital-gain, min-max scaled numerical features and one-hot encoded the object (categorical) features. Next we shuffled the dataset and split it into a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target into train and test data\n",
    "# Don't use test set for model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_final, income, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set has {} samples'.format(X_train.shape[0]))\n",
    "print('Testing set has {} samples'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying models and evaluating performace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline - Naive Predictor\n",
    "\n",
    "We first calculated accuracy, recall, precision and the F1 score for a naive predictor as a baseline for the performance measurement of the classification methods. The naive predictor classifies all entries as Positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True positives without False Negatives - sum the ones\n",
    "TP = np.sum(income)\n",
    "# False positives without True negatives - sum all and subtract from TP\n",
    "FP = income.count() - TP\n",
    "# There are no True Negatives nor False Negatives\n",
    "TN = FN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "recall = TP / (TP + FN)\n",
    "precision = TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score\n",
    "beta = 1.0\n",
    "fscore = (1 + beta**2) * (precision * recall) / ( recall + (precision * beta**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive predictor: [Accuracy Score: {:.4f}, F-score: {:.4f}]\".format(accuracy, fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Supervised Learning Models\n",
    "\n",
    "We now chose five models for training and assessed which one had a better performance classifying the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, accuracy_score, make_scorer, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function trains and evaluates a model using a defined sample size.\n",
    "\n",
    "INPUT:\n",
    "    model - instantiated sklearn model\n",
    "    sample_size - number of entries to be taken from training set\n",
    "    X_train - numpy array or pandas dataframe with training features\n",
    "    y_train - numpy array or pandas dataframe with target values\n",
    "    \n",
    "OUTPUT:\n",
    "    results - dictionary containing the performance parameters for the model\n",
    "    \n",
    "'''\n",
    "\n",
    "def trainPredict(model, sample_size, X_train, y_train):\n",
    "    results = {}\n",
    "    \n",
    "    # shuffle training and target data and return array with n_samples elements\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train.values, \n",
    "                                                 y_train.values, \n",
    "                                                 n_samples = sample_size)\n",
    "    \n",
    "    # fit training data and measure time\n",
    "    start = time()\n",
    "    model.fit(X_train_shuffled, y_train_shuffled)\n",
    "    end = time()\n",
    "    results['train_time'] = end - start\n",
    "    \n",
    "    # predict using the first 300 elements of training set and measure time\n",
    "    start = time()\n",
    "    predictions_train = model.predict(X_train[:300])\n",
    "    # evaluate accuracy and F1 scores using 3-fold cross-validation\n",
    "    # I cannot use X_test here because this is still a step of model selection\n",
    "    accuracy_crossval = cross_val_score(model, \n",
    "                                        X_train_shuffled, \n",
    "                                        y_train_shuffled, \n",
    "                                        cv=3, \n",
    "                                        scoring='accuracy')\n",
    "    f1_crossval = cross_val_score(model, \n",
    "                                  X_train_shuffled, \n",
    "                                  y_train_shuffled, \n",
    "                                  cv=3, \n",
    "                                  scoring='f1')\n",
    "    end = time()\n",
    "    results['pred_time'] = end - start\n",
    "    \n",
    "    # calculate accuracy using the first 300 entries of training set\n",
    "    results['acc_train'] = accuracy_score(y_train[:300], predictions_train)\n",
    "    # average accuracy score calculated using 3-fold cross validation\n",
    "    results['acc_cv'] = accuracy_crossval.mean()\n",
    "    # compute F1 score using the first 300 samples of training set\n",
    "    results['f1_train'] = fbeta_score(y_train[:300], predictions_train, beta=1.0)\n",
    "    # average F1 score calculated using 3-fold cross validation\n",
    "    results['f1_cv'] = f1_crossval.mean()\n",
    "    print(\"{} trained on {} samples\".format(model.__class__.__name__, sample_size))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import supervised learning models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intantiate the models\n",
    "clf_A = DecisionTreeClassifier(random_state=34)\n",
    "clf_B = AdaBoostClassifier(random_state=34)\n",
    "clf_C = LogisticRegression(random_state=34)\n",
    "clf_D = RandomForestClassifier(random_state=34)\n",
    "clf_E = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of entries corresponding to 1%, 10%, 100% of total entries\n",
    "# 1% of training entries\n",
    "samples_1 = int(0.01 * X_train.shape[0]) \n",
    "# 10% of training entries\n",
    "samples_10 = int(0.1 * X_train.shape[0])\n",
    "# 100% of training entries\n",
    "samples_100 = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results on the learners\n",
    "results = {}\n",
    "\n",
    "for clf in [clf_A, clf_B, clf_C, clf_D, clf_E]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name]={}\n",
    "    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "        results[clf_name][i] = trainPredict(clf, samples, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics visualization for the supervised learning process\n",
    "# first convert dict to pandas dataframe\n",
    "df2 = pd.DataFrame.from_dict({(i,j): results[i][j] for i in results.keys() \n",
    "                              for j in results[i].keys()}, orient='index')\n",
    "df2.reset_index(inplace=True)\n",
    "d2 = {0:'1%',1:'10%',2:'100%'}\n",
    "df2['dataP'] = df2['level_1'].map(d2)\n",
    "df2.rename(index=str, columns={'level_0':'model'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then plot using Seaborn\n",
    "with sns.axes_style('whitegrid',{'axes.facecolor':'white'}):\n",
    "    f, axes = plt.subplots(2,3, figsize=(14,8))\n",
    "    plt.suptitle('Performance Metrics', y=1.1, fontsize=18)\n",
    "    sns.barplot(x='dataP',y='train_time',hue='model',data=df2, ax=axes[0][0])\n",
    "    axes[0][0].set_title('Model Training')\n",
    "    axes[0][0].set_xlabel('Training Set Size')\n",
    "    axes[0][0].set_ylabel('Time (s)')\n",
    "    axes[0][0].get_legend().set_visible(False)\n",
    "    sns.barplot(x='dataP',y='acc_train',hue='model',data=df2, ax=axes[0][1])\n",
    "    axes[0][1].set_title('Accuracy Score on Training Subset')\n",
    "    axes[0][1].set_xlabel('Training Set Size')\n",
    "    axes[0][1].set_ylabel('Accuracy Score')\n",
    "    axes[0][1].get_legend().set_visible(False)\n",
    "    sns.barplot(x='dataP',y='f1_train',hue='model',data=df2, ax=axes[0][2])\n",
    "    axes[0][2].set_title('F Score on Training Subset')\n",
    "    axes[0][2].set_xlabel('Training Set Size')\n",
    "    axes[0][2].set_ylabel('F Score')\n",
    "    axes[0][2].get_legend().set_visible(False)\n",
    "    sns.barplot(x='dataP',y='pred_time',hue='model',data=df2, ax=axes[1][0])\n",
    "    axes[1][0].set_title('Model Prediction')\n",
    "    axes[1][0].set_xlabel('Training Set Size')\n",
    "    axes[1][0].set_ylabel('Time (s)')\n",
    "    axes[1][0].get_legend().set_visible(False)\n",
    "    sns.barplot(x='dataP',y='acc_cv',hue='model',data=df2, ax=axes[1][1])\n",
    "    axes[1][1].set_title('Accuracy Score on Cross Validation')\n",
    "    axes[1][1].set_xlabel('Training Set Size')\n",
    "    axes[1][1].set_ylabel('Accuracy Score')\n",
    "    axes[1][1].get_legend().set_visible(False)\n",
    "    sns.barplot(x='dataP',y='f1_cv',hue='model',data=df2, ax=axes[1][2])\n",
    "    axes[1][2].set_title('F Score on Cross Validation')\n",
    "    axes[1][2].set_xlabel('Training Set Size')\n",
    "    axes[1][2].set_ylabel('F Score')\n",
    "    axes[1][2].get_legend().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='center left',bbox_to_anchor=(1.0,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect actual training parameters for the models\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost had the best cross-validation performance with the full dataset. Decision Tree performed well with the small dataset (300 entries) and logistic regression performed surprisingly well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModel(clf, df, col1, col2, ax, nPlot):\n",
    "#def plotModel(clf, df, col1, col2, nPlot):\n",
    "    nPoints = nPlot**2\n",
    "    df2 = df[0:nPoints].copy()\n",
    "    X = df2[col1]\n",
    "    Y = df2[col2]\n",
    "    x_min, x_max = X.min() , X.max()\n",
    "    y_min, y_max = Y.min() , Y.max() \n",
    "    xRange = np.linspace(x_min, x_max, num=nPlot)\n",
    "    yRange = np.linspace(y_min, y_max, num=nPlot)\n",
    "    xx, yy = np.meshgrid(xRange, yRange)\n",
    "    df3 = pd.DataFrame(list(product(xx.ravel(),yy.ravel())), columns=[col1,col2])\n",
    "    df2.update(df3[col1])\n",
    "    df2.update(df3[col2])\n",
    "    Z = clf.predict(df2)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "#    fig, ax = plt.subplots(1)\n",
    "    ax.set_title(str(clf.__class__.__name__))\n",
    "    ax.set_xlabel(col1)\n",
    "    ax.set_ylabel(col2)\n",
    "    ax.contourf(xx, yy, Z, cmap = plt.cm.RdYlBu, alpha=0.5)\n",
    "    scFig = ax.scatter(df2.iloc[0:nPoints,df2.columns.get_loc(col1)], \n",
    "           df2.iloc[0:nPoints,df2.columns.get_loc(col2)], c=y_train[0:nPoints], cmap='RdYlBu', s=2)\n",
    "    return scFig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = np.array(['age', 'education-num','capital-gain','capital-loss','hours-per-week'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**We inspected the shape of the decision boundaries when projected on the planes defined by pairs of numerical variables for AdaBoost and Logistic Regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(int(len(numerical)),2,figsize=(12,20))\n",
    "\n",
    "i_n = 0\n",
    "j_n = 0\n",
    "for i in numerical:\n",
    "    for j in numerical:\n",
    "        if i < j:\n",
    "            plotModel(clf_B, X_train, i, j, axes[i_n][j_n], 20)\n",
    "            j_n += 1\n",
    "            if j_n == 2:\n",
    "                j_n = 0\n",
    "                i_n += 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(int(len(numerical)),2,figsize=(12,20))\n",
    "\n",
    "i_n = 0\n",
    "j_n = 0\n",
    "for i in numerical:\n",
    "    for j in numerical:\n",
    "        if i < j:\n",
    "            plotModel(clf_C, X_train, i, j, axes[i_n][j_n], 20)\n",
    "            j_n += 1\n",
    "            if j_n == 2:\n",
    "                j_n = 0\n",
    "                i_n += 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "-  AdaBoost received the best accuracy and F1 scores for the full training set. However, it was also the most time-consuming method for training by a large margin.\n",
    "-  Decision Tree received the best scores for a small training set but it is badly overfitting.\n",
    "-  Logistic regression demanded much less resources for training with the full dataset but it received scores similar to AdaBoost and higher than Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function plots the learning curve for a given model.\n",
    "\n",
    "INPUT:\n",
    "    model - instantiated sklearn model\n",
    "    X - numpy array or pandas dataframe with training features\n",
    "    y - numpy array or pandas dataframe with target values\n",
    "    \n",
    "OUTPUT:\n",
    "    graph with learning curves (Training scores and Testing scores)\n",
    "    \n",
    "'''\n",
    "def plotLearning(model,X,y):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model,\n",
    "                                                            X,\n",
    "                                                            y,\n",
    "                                                            cv=3,\n",
    "                                                            n_jobs=-1,\n",
    "                                                            shuffle=True,\n",
    "                                                       train_sizes=np.linspace(.1, 1.0, 10),\n",
    "                                                           scoring='f1')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    \n",
    "    ax.grid()\n",
    "    ax.fill_between(train_sizes, \n",
    "                 train_scores_mean - train_scores_std, \n",
    "                 train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, \n",
    "                 color='r')\n",
    "    \n",
    "    ax.set_xlabel('Elements in Training Set')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_title(str(model.__class__.__name__))\n",
    "    ax.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training scores')\n",
    "    plt.legend()\n",
    "    ax.fill_between(train_sizes,\n",
    "                test_scores_mean - test_scores_std,\n",
    "                test_scores_mean + test_scores_std,\n",
    "                alpha=0.1,\n",
    "                color='b')\n",
    "    ax.plot(train_sizes, test_scores_mean, 'o-', color='b', label='Testing scores')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves for all models\n",
    "for clf in [clf_A, clf_B, clf_C, clf_D, clf_E]:\n",
    "    plotLearning(clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:** \n",
    "\n",
    "-  F1 score was used for learning curves because accuracy is not a good metric for this problem.\n",
    "-  Decision Tree is overfitting\n",
    "-  AdaBoost and Logistic Regression show balanced learning curves. However, the score for AdaBoost is higher.\n",
    "-  Learning curve for GaussianNB showed improved with the number of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosen model:\n",
    "\n",
    "AdaBoost is the chosen model for this problem. It returned the best F1 score with the complete training set.\n",
    "\n",
    "-  accuracy score with full training set: 0.830000 \t\n",
    "-  F1 score with full training set: 0.67418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_raw = clf_B\n",
    "acc_raw = cross_val_score(clf_raw, X_train, y_train, cv=3, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_raw = cross_val_score(clf_raw, X_train, y_train, cv=3, scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy score was {:.3f} and F1 score was {:.3f} for raw AdaBoost.'.format(acc_raw, f1_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Improving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "\n",
    "We performed first random search to probe different regions of the configuration space and to define a subset of parameters for a more detailed search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for random search\n",
    "parameters = {\"n_estimators\": np.arange(100,2000,100),\n",
    "            \"learning_rate\": np.arange(0.1,1.2,0.1),\n",
    "              \"algorithm\":['SAMME','SAMME.R'],\n",
    "              \"base_estimator\": [DecisionTreeClassifier(max_depth=1),DecisionTreeClassifier(max_depth=2),\n",
    "            DecisionTreeClassifier(max_depth=3)]  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 60 iterations of random search\n",
    "random_obj = RandomizedSearchCV(clf_raw, param_distributions=parameters, n_iter = 60)\n",
    "random_obj.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best random search estimator\n",
    "clf_random = random_obj.best_estimator_\n",
    "clf_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of random estimator\n",
    "acc_random = random_obj.best_score_\n",
    "acc_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score for random estimator\n",
    "f1_random = cross_val_score(clf_random, X_train, y_train, cv=3, scoring='f1').mean()\n",
    "f1_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Raw model')\n",
    "print('Accuracy score was {:.3f} and F1 score was {:.3f} for raw AdaBoost.'.format(acc_raw, f1_raw))\n",
    "print('Random search model')\n",
    "print('Accuracy score was {:.3f} and F1 score was {:.3f} for raw AdaBoost.'.format(acc_random, f1_random))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The random model improved the raw model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "We now performed grid search around the best result found by random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"n_estimators\": np.arange(1400,1600,20),\n",
    "            \"learning_rate\": np.arange(0.6,0.8,0.02)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a F1 scorer that will evaluate the quality of solutions found in the grid\n",
    "scorer = make_scorer(fbeta_score, beta=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a object for grid search\n",
    "grid_obj = GridSearchCV(clf_random, param_grid = parameters, cv=3, scoring=scorer, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid on training set\n",
    "grid_obj.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the instance with the best perfomance\n",
    "clf_grid = grid_obj.best_estimator_\n",
    "clf_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 score improve\n",
    "f1_grid = grid_obj.best_score_\n",
    "f1_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_grid = cross_val_score(clf_grid, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "acc_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Raw model')\n",
    "print('Accuracy score was {:.4f} and F1 score was {:.4f} for raw AdaBoost.'.format(acc_raw, f1_raw))\n",
    "print('Random search model')\n",
    "print('Accuracy score was {:.4f} and F1 score was {:.4f} for raw AdaBoost.'.format(acc_random, f1_random))\n",
    "print('Grid search model')\n",
    "print('Accuracy score was {:.4f} and F1 score was {:.4f} for raw AdaBoost.'.format(acc_grid, f1_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was a slight improvement using grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_calibr = CalibratedClassifierCV(clf_grid, cv=3)\n",
    "clf_calibr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_calibr = cross_val_score(clf_calibr, X_train, y_train, cv=3).mean()\n",
    "acc_calibr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_calibr = cross_val_score(clf_calibr, X_train, y_train, cv=3, scoring='f1').mean()\n",
    "f1_calibr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibr_proba = cross_val_predict(clf_calibr, X_train, y_train, cv=3, method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fop, mpv = calibration_curve(y_train, calibr_proba[:,1], n_bins=10, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mpv, fop, marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Raw model')\n",
    "print('Accuracy score was {:.4f} and F1 score was {:.4f} for raw AdaBoost.'.format(acc_raw, f1_raw))\n",
    "print('Random search model')\n",
    "print('Accuracy score was {:.4f} and F1 score was {:.4f} for raw AdaBoost.'.format(acc_random, f1_random))\n",
    "print('Grid search model')\n",
    "print('Accuracy score was {:.4f} and F1 score was {:.4f} for raw AdaBoost.'.format(acc_grid, f1_grid))\n",
    "print('Calibrated model')\n",
    "print('Accuracy score was {:.4f} and F1 score was {:.4f} for raw AdaBoost.'.format(acc_calibr, f1_calibr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calibrated model slightly underperformed when compared to the grid model. We therefore take the grid model as the best model but we will also test the calibrated model with the test data to check if it generalizes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best = clf_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Evaluation of the best model\n",
    "\n",
    "We will now apply tests to evaluate the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best_proba = cross_val_predict(clf_best, X_train, y_train, cv=3, method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_train, clf_best_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = auc(fpr, tpr)\n",
    "print('The area under the ROC curve is: ',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr);\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve shows that the model is better than random guessing but the classifier is far from perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_proba = clf_best.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_train_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recalls[:-1], 'g--', label=\"Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold is located at 0.5. Was that obvious?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation score\n",
    "\n",
    "Now we calculated the p-value for the F1 score calculated for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, permutation_scores, pvalue = permutation_test_score(\n",
    "    clf_best, X_train, y_train, scoring=\"f1\", cv=3, n_permutations=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is calculated as (C + 1) / (n_permutations + 1) where C is the number of permutations where the score is higher than the true score. Therefore the best possible value is 1 / (n_permutations + 1) = 1 / 11 = 0.0909. We conclude that the score calculated is significative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(clf_best, X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix shows a high number of false negatives. This certainly demands some investigation if some aspect of data cleaning was overlooked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_A = DecisionTreeClassifier(random_state=34)\n",
    "clf_B = AdaBoostClassifier(random_state=34)\n",
    "clf_C = LogisticRegression(random_state=34)\n",
    "clf_D = RandomForestClassifier(random_state=34)\n",
    "clf_E = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_voting = VotingClassifier(estimators=[('calibr', clf_calibr),\n",
    "#                                          ('logReg', clf_C)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting soft requires calibrated classifiers\n",
    "clf_voting = VotingClassifier(estimators=[('calibr', clf_calibr),('logReg', clf_C)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_voting.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_voting = cross_val_score(clf_voting, X_train, y_train, cv=3, scoring='f1').mean()\n",
    "f1_voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement here. Classifier clf_grid is still the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring with Testing set\n",
    "\n",
    "Now we finally applied the test set to assess the best classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = clf_best.score(X_test, y_test) # this is accuracy\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = f1_score(y_test, clf_best.predict(X_test))\n",
    "f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, clf_calibr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_raw_test = clf_raw.score(X_test, y_test) # this is accuracy\n",
    "acc_raw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_raw_test = f1_score(y_test, clf_raw.predict(X_test)) # this is accuracy\n",
    "f1_raw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"Accuracy score on testing data: {:.4f}\".format(acc_raw_test))\n",
    "print(\"F-score on testing data: {:.4f}\".format(f1_raw_test))\n",
    "print(\"\\nOptimized optimized Model\\n------\")\n",
    "print(\"Final accuracy score on the testing data: {:.4f}\".format(acc_test))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Feature Importance\n",
    "\n",
    "An important task when performing supervised learning on a dataset like the census data we study here is determining which features provide the most predictive power. By focusing on the relationship between only a few crucial features and the target label we simplify our understanding of the phenomenon, which is most always a useful thing to do. In the case of this project, that means we wish to identify a small number of features that most strongly predict whether an individual makes at most or more than \\$50,000.\n",
    "\n",
    "Choose a scikit-learn classifier (e.g., adaboost, random forests) that has a `feature_importance_` attribute, which is a function that ranks the importance of features according to the chosen classifier.  In the next python cell fit this classifier to training set and use this attribute to determine the top 5 most important features for the census dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataframe containing feature labels and their weights\n",
    "feature_importances = pd.DataFrame(clf_best.feature_importances_, \n",
    "                                   index = X_train.columns, \n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cumulative sum of weights as a separate column\n",
    "feature_importances['cumsum'] = np.cumsum(feature_importances['importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = clf_best.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('whitegrid',{'axes.facecolor':'white'}):\n",
    "    fig, axis = plt.subplots(1,1, figsize=(8,6))\n",
    "    plt.suptitle('Normalized Weights for First Five Most Predictive Features')\n",
    "    sns.barplot(x='index',\n",
    "                y='importance',\n",
    "                data=feature_importances.iloc[:5], \n",
    "                ax=axis, \n",
    "                color='blue', \n",
    "                ci=None, label='Weight')\n",
    "    sns.barplot(x='index',\n",
    "                y='cumsum',\n",
    "                data=feature_importances.iloc[:5], \n",
    "                ax=axis, \n",
    "                color='red', \n",
    "                alpha=0.5, \n",
    "                ci=None, label='Cumulative Weight')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    axis.set_xlabel('Feature')\n",
    "    axis.set_ylabel('Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capital gain and capital loss are the features that are the most important to explain the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colect the first five columns in descending order of importance\n",
    "X_train_reduced = X_train[X_train.columns.values[np.argsort(importance)[::-1][:5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone best model obtained above and train it with reduced training set\n",
    "clf_fs = (clone(clf_best)).fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with reduced test set\n",
    "f1_fs = cross_val_score(clf_fs, X_train_reduced, y_train, cv=3, scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_fs = cross_val_score(clf_fs, X_train_reduced, y_train, cv=3, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report scores from the final model using both versions of data\n",
    "print(\"Final Model trained on full data\\n------\")\n",
    "print(\"Accuracy on CV: {:.4f}\".format(acc_grid))\n",
    "print(\"F-score on CV: {:.4f}\".format(f1_grid))\n",
    "print(\"\\nFinal Model trained on reduced data\\n------\")\n",
    "print(\"Accuracy on CV: {:.4f}\".format(acc_fs))\n",
    "print(\"F-score on CV: {:.4f}\".format(f1_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### RFE feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RFE = RFE(clf_best, 5, step = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RFE.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns[clf_RFE.support_].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rfe = clf_RFE.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rfe = cross_val_score(clf_RFE, X_train, y_train, cv=3, scoring='f1').mean()\n",
    "f1_rfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_kb = (clone(clf_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = SelectKBest(chi2, k=5).fit_transform(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_kb.fit(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf_kb, X_train2, y_train, cv=3, scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.fit_transform(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf_best, X_pca, y_train, cv=3, scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump_session('finding_donors.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "-  Feature selection improved - done\n",
    "-  Plot classification results - done\n",
    "-  calculate p-value for score - done\n",
    "-  calculate p-value for coefficients of logistic regression\n",
    "-  ROC curve\n",
    "-  AdaBoost Grid SearchCV - try optmizing trees - done\n",
    "- print adaboost tree - not possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO - Naive Bayes\n",
    "\n",
    "-  sklearn.calibration.CalibratedClassifierCV\n",
    "   -   https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV\n",
    "-  multinomialNB for categorical and GaussianNB for numerical. Multiply predict_proba or use probabilities as input ofr another GaussianNB classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
